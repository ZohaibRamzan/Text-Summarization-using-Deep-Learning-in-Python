{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Abstractive Text Summarization using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "“I don’t want to read your full report, just give me a summary of the results”. I have often found myself in this situation – both in University as well as in our professional life, We prepare a comprehensive report and the teacher/supervisor only has time to read the summary.\n",
    "\n",
    "We all think so? Well, I decided to do something about it. Manually converting the report to a summarized version is too time taking, right? Can Natural Language Processing (NLP) techniques help us in achieving this?\n",
    "\n",
    "This is where the awesome concept of Text Summarization using Deep Learning really helped me out. It solves the one issue which kept bothering me before – now our model can understand the context of the entire text. It’s a dream come true for all of us who need to come up with a quick summary of a document!\n",
    "And the results we achieve using text summarization in deep learning? Remarkable. So in this article, we will walk through a step-by-step process for building a Text Summarizer using Deep Learning by covering all the concepts required to build it. And then we will implement our first text summarization model in Python!\n",
    "\n",
    "## Table of Contents\n",
    "What is Text Summarization in NLP?\n",
    "<ol>\n",
    "    <li>Introduction to Sequence-to-Sequence (Seq2Seq) Modeling</li>\n",
    "<li>Understanding the Encoder – Decoder Architecture</li>\n",
    "<li>Limitations of the Encoder – Decoder Architecture</li>\n",
    "<li>The Intuition behind the Attention Mechanism</li>\n",
    "<li>Understanding the Problem Statement</li>\n",
    "<li>Implementing a Text Summarization Model in Python using Keras</li>\n",
    "</ol>\n",
    "\n",
    "## What’s Next?\n",
    "How does the Attention Mechanism Work?\n",
    "I’ve kept the ‘how does the attention mechanism work?’ section at the bottom of this article. It’s a math-heavy section and is not mandatory to understand how the Python code works. However, I encourage you to go through it because it will give you a solid idea of this awesome NLP concept.\n",
    "\n",
    "## What is Text Summarization in NLP?\n",
    "Let’s first understand what text summarization is before we look at how it works. Here is a succinct definition to get us started:\n",
    "\n",
    "“Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning”\n",
    "\n",
    "There are broadly two different approaches that are used for text summarization:\n",
    "\n",
    "<li>Extractive Summarization</li>\n",
    "<li>Abstractive Summarization</li>\n",
    "\n",
    "### Extractive Summarization\n",
    "The name gives away what this approach does. We identify the important sentences or phrases from the original text and extract only those from the text. Those extracted sentences would be our summary. The below diagram illustrates extractive summarization:\n",
    "\n",
    "![Extractive Summary](extractive1.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Abstractive Summarization\n",
    "This is a very interesting approach. Here, we generate new sentences from the original text. This is in contrast to the extractive approach we saw earlier where we used only the sentences that were present. The sentences generated through abstractive summarization might not be present in the original text:\n",
    "\n",
    "![title](abstractive1.jpg)\n",
    "\n",
    "\n",
    "we are going to build an Abstractive Text Summarizer using Deep Learning in this article! Let’s first understand the concepts necessary for building a Text Summarizer model before diving into the implementation part.\n",
    "\n",
    "Exciting times ahead!\n",
    "\n",
    "## Introduction to Sequence-to-Sequence (Seq2Seq) Modeling\n",
    "We can build a Seq2Seq model on any problem which involves sequential information. This includes Sentiment classification, Neural Machine Translation, and Named Entity Recognition – some very common applications of sequential information.\n",
    "\n",
    "In the case of Neural Machine Translation, the input is a text in one language and the output is also a text in another language:(English to German)\n",
    "\n",
    "#### <center> How are you? ---->  Wie geht es dir? </center>\n",
    "\n",
    "In the Named Entity Recognition, the input is a sequence of words and the output is a sequence of tags for every word in the input sequence:\n",
    "\n",
    "\n",
    "#### <center>Hafiz  Zohaib founded CommInn.  --->   B-Per, I-Per,O,B-Company,O</center>\n",
    "\n",
    "Our objective is to build a text summarizer where the input is a long sequence of words (in a text body), and the output is a short summary (which is a sequence as well). So, we can model this as a Many-to-Many Seq2Seq problem. Below is a typical Seq2Seq model architecture:\n",
    "\n",
    "![title](final.jpg)\n",
    "\n",
    "There are two major components of a Seq2Seq model:\n",
    "\n",
    "<li>Encoder</li>\n",
    "<li>Decoder</li>\n",
    "Let’s understand these two in detail. These are essential to understand how text summarization works underneath the code. You can also check out this tutorial to understand sequence-to-sequence modeling in more detail.\n",
    "\n",
    " \n",
    "\n",
    "## Understanding the Encoder-Decoder Architecture\n",
    "The Encoder-Decoder architecture is mainly used to solve the sequence-to-sequence (Seq2Seq) problems where the input and output sequences are of different lengths. Let’s understand this from the perspective of text summarization. The input is a long sequence of words and the output will be a short version of the input sequence.\n",
    "\n",
    "![title](first.jpg)\n",
    "\n",
    "\n",
    "Generally, variants of Recurrent Neural Networks (RNNs), i.e. Gated Recurrent Neural Network (GRU) or Long Short Term Memory (LSTM), are preferred as the encoder and decoder components. This is because they are capable of capturing long term dependencies by overcoming the problem of vanishing gradient.\n",
    "\n",
    "We can set up the Encoder-Decoder in 2 phases:\n",
    "\n",
    "<li>Training phase</li>\n",
    "<li>Inference phase</li>\n",
    "Let’s understand these concepts through the lens of an LSTM model.\n",
    "\n",
    " \n",
    "\n",
    "## Training phase\n",
    "In the training phase, we will first set up the encoder and decoder. We will then train the model to predict the target sequence offset by one timestep. Let us see in detail on how to set up the encoder and decoder.\n",
    "\n",
    " \n",
    "\n",
    "## Encoder\n",
    "\n",
    "An Encoder Long Short Term Memory model (LSTM) reads the entire input sequence wherein, at each timestep, one word is fed into the encoder. It then processes the information at every timestep and captures the contextual information present in the input sequence. I’ve put together the below diagram which illustrates this process:\n",
    "\n",
    "![title](61.jpg)\n",
    "\n",
    "\n",
    "The hidden state (hi) and cell state (ci) of the last time step are used to initialize the decoder. Remember, this is because the encoder and decoder are two different sets of the LSTM architecture.\n",
    "\n",
    " \n",
    "\n",
    "## Decoder\n",
    "\n",
    "The decoder is also an LSTM network which reads the entire target sequence word-by-word and predicts the same sequence offset by one timestep. The decoder is trained to predict the next word in the sequence given the previous word.\n",
    "\n",
    "![title](61.jpg)\n",
    "\n",
    "<b>start</b> and <b>end</b> are the special tokens which are added to the target sequence before feeding it into the decoder. The target sequence is unknown while decoding the test sequence. So, we start predicting the target sequence by passing the first word into the decoder which would be always the <start> token. And the <end> token signals the end of the sentence.\n",
    "\n",
    "Pretty intuitive so far.\n",
    "\n",
    "## Inference Phase\n",
    "    \n",
    "After training, the model is tested on new source sequences for which the target sequence is unknown. So, we need to set up the inference architecture to decode a test sequence:\n",
    "    \n",
    "![title](82.jpg)   \n",
    "    \n",
    "## How does the inference process work?\n",
    "\n",
    "Here are the steps to decode the test sequence:\n",
    "\n",
    "Encode the entire input sequence and initialize the decoder with internal states of the encoder\n",
    "Pass <start> token as an input to the decoder\n",
    "Run the decoder for one timestep with the internal states\n",
    "The output will be the probability for the next word. The word with the maximum probability will be selected\n",
    "Pass the sampled word as an input to the decoder in the next timestep and update the internal states with the current time step\n",
    "Repeat steps 3 – 5 until we generate <end> token or hit the maximum length of the target sequence\n",
    "    Let’s take an example where the test sequence is given by  <b>[x1, x2, x3, x4]</b>. How will the inference process work for this test sequence? I want you to think about it before you look at my thoughts below.\n",
    "<ol>\n",
    "    <li>Encode the test sequence into internal state vectors</li>\n",
    "    <li>Observe how the decoder predicts the target sequence at each timestep:</li>\n",
    "</ol>                   \n",
    "<b>Timestep: t=1</b>\n",
    "    \n",
    "![title](d1.jpg) \n",
    "    \n",
    "<b>Timestep: t=2</b>\n",
    "    \n",
    "![title](d2.jpg)  \n",
    "    \n",
    "<b>Timestep: t=3</b>\n",
    "    \n",
    "![title](d3.jpg) \n",
    "    \n",
    "## Limitations of the Encoder – Decoder Architecture\n",
    "As useful as this encoder-decoder architecture is, there are certain limitations that come with it.\n",
    "\n",
    "<li>The encoder converts the entire input sequence into a fixed length vector and then the decoder predicts the output sequence. This works only for short sequences since the decoder is looking at the entire input sequence for the prediction\n",
    "    </li>\n",
    "<li>Here comes the problem with long sequences. It is difficult for the encoder to memorize long sequences into a fixed length vector</li>\n",
    "“A potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences. The performance of a basic encoder-decoder deteriorates rapidly as the length of an input sentence increases.”\n",
    "\n",
    "         -Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "So how do we overcome this problem of long sequences? This is where the concept of attention mechanism comes into the picture. It aims to predict a word by looking at a few specific parts of the sequence only, rather than the entire sequence. It really is as awesome as it sounds!\n",
    "    \n",
    "## The Intuition behind the Attention Mechanism\n",
    "How much attention do we need to pay to every word in the input sequence for generating a word at timestep t? That’s the key intuition behind this attention mechanism concept.\n",
    "\n",
    "Let’s consider a simple example to understand how Attention Mechanism works:\n",
    "<ul>\n",
    "    <li>Source sequence: “Which sport do you like the most?</li>\n",
    "    <li>Target sequence: “I love cricket”</li>\n",
    "    </ul>\n",
    "The first word ‘I’ in the target sequence is connected to the fourth word ‘you’ in the source sequence, right? Similarly, the second-word ‘love’ in the target sequence is associated with the fifth word ‘like’ in the source sequence.\n",
    "\n",
    "So, instead of looking at all the words in the source sequence, we can increase the importance of specific parts of the source sequence that result in the target sequence. This is the basic idea behind the attention mechanism.\n",
    "\n",
    "There are 2 different classes of attention mechanism depending on the way the attended context vector is derived:\n",
    "<ul>\n",
    "    <li>Global Attention</li>\n",
    "    <li>Local Attention</li>\n",
    "    </ul>\n",
    "Let’s briefly touch on these classes.\n",
    "    \n",
    "## Global Attention\n",
    "Here, the attention is placed on all the source positions. In other words, all the hidden states of the encoder are considered for deriving the attended context vector.\n",
    "    \n",
    "    \n",
    "    \n",
    "## Local Attention\n",
    "Here, the attention is placed on only a few source positions. Only a few hidden states of the encoder are considered for deriving the attended context vector.\n",
    "![title](gl.png)    \n",
    "## Understanding the Problem Statement\n",
    "Customer reviews can often be long and descriptive. Analyzing these reviews manually, as you can imagine, is really time-consuming. This is where the brilliance of Natural Language Processing can be applied to generate a summary for long reviews.\n",
    "We will be working on a really cool dataset. Our objective here is to generate a summary for the Amazon Fine Food reviews using the abstraction-based approach we learned about above.\n",
    "    \n",
    "## Implementing Text Summarization in Python using Keras\n",
    "It’s time to fire up our Jupyter notebooks! Let’s dive into the implementation details right away.\n",
    "\n",
    "## Custom Attention Layer\n",
    "Keras does not officially support attention layer. So, we can either implement our own attention layer or use a third-party implementation. We will go with the latter option for this article. You can download the attention layer from here and copy it in a different file called attention.py.\n",
    "\n",
    "Let’s import it into our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset\n",
    "This dataset consists of reviews of fine foods from Amazon. The data spans a period of more than 10 years, including all ~500,000 reviews up to October 2012. These reviews include product and user information, ratings, plain text review, and summary. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "We’ll take a sample of 100,000 reviews to reduce the training time of our model. Feel free to use the entire dataset for training your model if your machine has that kind of computational power.\n",
    "You can find the reviews at https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         int64\n",
       "ProductId                 object\n",
       "UserId                    object\n",
       "ProfileName               object\n",
       "HelpfulnessNumerator       int64\n",
       "HelpfulnessDenominator     int64\n",
       "Score                      int64\n",
       "Time                       int64\n",
       "Summary                   object\n",
       "Text                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Reviews.csv\",nrows=100000)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicates and NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace=True)  #dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)   #dropping na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Performing basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move. So in this step, we will drop all the unwanted symbols, characters, etc. from the text that do not affect the objective of our problem.\n",
    "Here is the dictionary that we will use for expanding the contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define two different functions for preprocessing the reviews and generating the summary since the preprocessing steps involved in text and summary differ slightly.\n",
    "## a) Text Cleaning\n",
    "Let’s look at the first 10 reviews in our dataset to get an idea of the text preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labr...\n",
       "1             Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
       "2    This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with ...\n",
       "3    If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The fl...\n",
       "4                                                               Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
       "5    I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there wa...\n",
       "6    This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, ...\n",
       "7                                                               This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!\n",
       "8                                                                        Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\n",
       "9                                                                  This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the below preprocessing tasks for our data:\n",
    "<ol>\n",
    "    <li>Convert everything to lowercase</li>\n",
    "<li>Remove HTML tags</li>\n",
    "<li>Contraction mapping</li>\n",
    "<li>Remove (‘s)</li>\n",
    "<li>Remove any text inside the parenthesis ( )</li>\n",
    "<li>Eliminate punctuations and special characters</li>\n",
    "<li>Remove stopwords</li>\n",
    "<li>Remove short words</li>\n",
    "    </ol>\n",
    "Let’s define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text # remove html tags\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)   # remove the signs\n",
    "    newString = re.sub('\"','', newString)             # remove double commas\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")]) # replace contaction mappings   \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)   # replace 's at end of a word\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) # Only retain alphabtes, remove numbers and any special characters\n",
    "    tokens = [w for w in newString.split() if not w in stop_words] #remove stop_words\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()  # removespace if exists at start or end of a word/string: ' banana ' --> 'banana'\n",
    "\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t))\n",
    "#print(type(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Summary Cleaning\n",
    "And now we’ll look at the first 10 rows of the reviews to an idea of the preprocessing steps for the summary column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Good Quality Dog Food\n",
       "1                                Not as Advertised\n",
       "2                            \"Delight\" says it all\n",
       "3                                   Cough Medicine\n",
       "4                                      Great taffy\n",
       "5                                       Nice Taffy\n",
       "6    Great!  Just as good as the expensive brands!\n",
       "7                           Wonderful, tasty taffy\n",
       "8                                       Yay Barley\n",
       "9                                 Healthy Dog Food\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Summary'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString\n",
    "\n",
    "#Call the above function\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(summary_cleaner(t))\n",
    "\n",
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True) # ?\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to add the START and END special tokens at the beginning and end of the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x : '_START_ '+ x + ' _END_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s take a look at the top 5 reviews and their summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summary: _START_ good quality dog food  _END_\n",
      "\n",
      "\n",
      "Review: product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summary: _START_ not as advertised  _END_\n",
      "\n",
      "\n",
      "Review: confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "Summary: _START_ delight says it all  _END_\n",
      "\n",
      "\n",
      "Review: looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal\n",
      "Summary: _START_ cough medicine  _END_\n",
      "\n",
      "\n",
      "Review: great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "Summary: _START_ great taffy  _END_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review:\",data['cleaned_text'][i])\n",
    "    print(\"Summary:\",data['cleaned_summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the distribution of the sequences\n",
    "Here, we will analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3df5Ac5X3n8ffH4ocJmJNkYI0FzsqJ7DIghx860BVOsgZbCDmJ8JVxpOKQ+HElmxJVxlY5LI7rRMBUwDGQ4MM4otAhORih8MMoRlheK8wR6hAgsAwIgbVgBRZ0UkDix4KNT/L3/uhnUWu2Z3d2Z3dmdvR5VXXN9Lef7nkebY++093P062IwMzM9m3va3QFzMys8ZwMzMzMycDMzJwMzMwMJwMzM8PJwMzMcDIwMzOcDMysyUnaIukzI7CdWyV9ayTq1IqcDKxqkvZrdB3MbHQ4GdSZpEslvSzpLUnPSTq9/BeLpA5JPbn5LZK+LulJSW9LukVSm6T703Z+JmlCKtsuKSSdL+klSTslfVnSf07rvy7pf+a2/QeS/lXSa5JelXSbpPFln32ppCeBt1M97ipr03cl/f2o/sPZPknSD4CPAP8iqVfSX0maLun/pH35F5I6UtmJknok/XmaP0RSt6R5khYA5wB/lbbzLw1rVLOKCE91moCPAy8BH07z7cAfALcC38qV6wB6cvNbgHVAGzAJ2A48AZwAHAj8K7A4t80Avg+8H5gB/Ab4EXBEbv0/TeX/EPhs2s7hwIPA35d99gbgaOAg4EjgbWB8Wr5f2t5Jjf739dSaU9oHP5PeTwJeA2aR/Zj9bJo/PC2fAfzftK/fDNyZ285e3zNPe08+Mqiv3WT/6R4jaf+I2BIRz1e57ncjYltEvAz8G/BIRPw8It4F7iFLDHlXRsRvIuKnZP953x4R23PrnwAQEd0R0RUR70bEfwDXAX9atq0bIuKliPh1RGwlSxhnp2UzgVcj4vEh/UuYDc9/A1ZHxOqI+F1EdAHryZIDaX//Z2At8DngSw2r6RjjZFBHEdENXAJcDmyXtELSh6tcfVvu/a8L5g8ZTnlJR6R6vCzpTeCfgMPKtvVS2fwysi8l6fUHVbbBrFa/D5ydThG9Lul14FNkR6x9lgDHAf8rIl5rRCXHIieDOouIH0bEp8h26gCuIfvl/nu5Yh+qY5X+NtXjkxFxKNl/7iorU35r2x8Bn5R0HPBnwG2jXkvbl+X3v5eAH0TE+Nx0cERcDSBpHPCPwHLgIkl/WGE7VsbJoI4kfVzSaZIOJDuP/2uyU0cbgFnpAtiHyI4e6uUDQC/wuqRJwNcHWyEifgPcCfwQeDQiXhzdKto+bhvw0fT+n4A/l3SGpHGS3p86XByVln8jvV4AfAdYnhJE+XasjJNBfR0IXA28yp6LXN8gO83yC7ILZT8F7qhjnf4GOBF4A7gPuLvK9ZYBU/EpIht9fwt8M50S+ktgNtn35j/IjhS+DrxP0knA14B5EbGb7Kg7gM60nVvIrte9LulHdW5D01O6ym42JJI+AjwLfCgi3mx0fcysNj4ysCGT9D6yX2ArnAjMWoNHlNqQSDqY7Nzrv5N1KzWzFuDTRGZm5tNEZmZWxWkiSUeT9dn9EPA7YElE/IOkiWS9XtrJesF8MSJ2ShLwD2QjAt8BzouIJ9K25gPfTJv+VkQsS/GTyIaKHwSsBr4SgxyyHHbYYdHe3j6UtjbE22+/zcEHH9zoaoyIVmvLs88++2pEHN7oulSr0j7fSn+Xcm7byHv88ceL9/vB7ldBNrLvxPT+A8AvgWOAbwOdKd4JXJPezwLuJxu4NJ3stgkAE4EX0uuE9H5CWvYo8F/SOvcDZw5Wr5NOOinGggceeKDRVRgxrdYWYH00wT1hqp0q7fOt9Hcp57aNvEr7/aCniSJia6Rf9hHxFrCJ7GZRs8n6mpNez0rvZwPL0+euA8ZLOhI4A+iKiB0RsRPoAmamZYdGxMOpostz2zIzszoYUm8iSe1kNzh7BGiL7KZlRMRWSUekYpPY+142PSk2ULynIF70+QuABQBtbW2USqWhVL8hent7x0Q9q9FqbTGzPapOBpIOAe4CLomIN7NLA8VFC2IxjHj/YMQSsptQMW3atOjo6Bik1o1XKpUYC/WsRqu1xcz2qKo3kaT9yRLBbRHRd7uCbekUD+l1e4r3kN37vs9RwCuDxI8qiJuZWZ0MmgxS76BbgE0RcV1u0Spgfno/H7g3F5+nzHTgjXQ6aQ0wQ9KE9FSuGcCatOyt9PQiAfNy2zIzszqo5jTRqcC5wFOSNqTYN8huuLZS0oXAi+x52Mlqsh5F3WRdS88HiIgdkq4EHkvlroiIHen9RezpWnp/mszMrE4GTQYR8RDF5/UBTi8oH8DCCttaCiwtiK8nexiFmZk1gEcgm5mZk4GZme0jyaC98z7aO+9rdDXMRpX3c6vFPpEMzMxsYE4GZmbmZGBmZk4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZj1I+loSQ9I2iRpo6SvpPjlkl6WtCFNs3LrXCapW9Jzks7IxWemWLekzlx8sqRHJG2WdIekA+rbSrO9DZoMJC2VtF3S07nYHbkvxJa+ZyNLapf069yy7+fWOUnSU+lLcYMkpfhESV3pS9ElacJoNNRsCHYBiyLiE8B0YKGkY9Ky6yPi+DStBkjL5gDHAjOB70kaJ2kccCNwJnAMMDe3nWvStqYAO4EL69U4syLVHBncSraDvyci/rLvCwHcBdydW/x87svy5Vz8JmABMCVNfdvsBNamL8XaNG/WMBGxNSKeSO/fAjYBkwZYZTawIiLejYhfAd3AyWnqjogXIuK3wApgdvohdBpwZ1p/GXDW6LTGrDr7DVYgIh6U1F60LO3UXyTbsSuSdCRwaEQ8nOaXk+3895N9kTpS0WVACbi0msqbjba0758APAKcClwsaR6wnuzoYSdZoliXW62HPcnjpbL4KcAHgdcjYldB+fLPX0D2I4q2tjZKpVK/Mr29vZRKJRZNzTZXVGas6mtbK2q2tg2aDAbxx8C2iNici02W9HPgTeCbEfFvZDt6T65Mfudvi4itkP0ik3REpQ+r5otRpJFfkmb7g9ei1doyGEmHkB35XhIRb0q6CbgSiPR6LXABoILVg+Ij7xigfP9gxBJgCcC0adOio6OjX5lSqURHRwfnpUdebjmnf5mxqq9trajZ2lZrMpgL3J6b3wp8JCJek3QS8CNJxzKEnX8g1XwxijTyS9Jsf/BatFpbBiJpf7JEcFtE3A0QEdtyy28Gfpxme4Cjc6sfBbyS3hfFXwXGS9ovHR3ky5s1xLB7E0naD/ivwB19sXTO9LX0/nHgeeBjZF+Wo3Kr53f+bek0Ut/ppO3DrZPZSEinP28BNkXEdbn4kblinwf6OlWsAuZIOlDSZLJrYo8CjwFTUs+hA8guMq+KiAAeAL6Q1p8P3DuabTIbTC1dSz8DPBsR753+kXR46kGBpI+SfSleSKeB3pI0PX3R5rFn519F9mUAfymsOZwKnAucVtaN9NupR9yTwKeBrwJExEZgJfAM8BNgYUTsTr/6LwbWkF2EXpnKQnZd7GuSusmuIdxSx/aZ9TPoaSJJt5Nd4D1MUg+wOCJuIfuVc3tZ8T8BrpC0C9gNfDkidqRlF5H1TDqI7MLx/Sl+NbBS0oXAi8DZtTTIrFYR8RDFpzZXD7DOVcBVBfHVRetFxAtkvY3MmkI1vYnmVoifVxC7i+w8a1H59cBxBfHXgNMHq4eZVdaerouZDZdHIJuZmZOBmZk5GZiZGftwMmjvvM/nWc3Mkn02GZiZ2R5OBmZm5mRgZmZOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmVJEMJC2VtF3S07nY5ZJeLntYeN+yyyR1S3pO0hm5+MwU65bUmYtPlvSIpM2S7pB0wEg20MzMBlfNkcGtwMyC+PURcXyaVgNIOgaYAxyb1vmepHGSxgE3AmcCxwBzU1mAa9K2pgA7gQtraZCZmQ3doMkgIh4EdlS5vdnAioh4NyJ+BXQDJ6epOyJeiIjfAiuA2ZIEnAbcmdZfBpw1xDaYmVmN9qth3YslzQPWA4siYicwCViXK9OTYgAvlcVPAT4IvB4RuwrK9yNpAbAAoK2tjVKpVFVFF03NNp8vXxQbDb29vaP+GfXSam0xsz2GmwxuAq4EIr1eC1wAqKBsUHwEEgOULxQRS4AlANOmTYuOjo6qKnteerzllnM6BoyNhlKpRLX1bHat1hYz22NYySAitvW9l3Qz8OM02wMcnSt6FPBKel8UfxUYL2m/dHSQL29mZnUyrK6lko7MzX4e6OtptAqYI+lASZOBKcCjwGPAlNRz6ACyi8yrIiKAB4AvpPXnA/cOp05mZjZ8gx4ZSLod6AAOk9QDLAY6JB1PdkpnC/AlgIjYKGkl8AywC1gYEbvTdi4G1gDjgKURsTF9xKXACknfAn4O3DJirTMzs6oMmgwiYm5BuOJ/2BFxFXBVQXw1sLog/gJZbyMzM2sQj0A2MzMnA7Nyko6W9ICkTZI2SvpKik+U1JVGy3dJmpDiknRDGl3/pKQTc9uan8pvljQ/Fz9J0lNpnRvSmBuzhnEyMOtvF9nYmU8A04GFacR8J7A2jZZfm+YhG1k/JU0LyLpeI2ki2TW2U8hOhS7uSyCpzILcekWj/M3qxsnArExEbI2IJ9L7t4BNZIMhZ5ONkoe9R8vPBpZHZh1Zd+kjgTOArojYkQZldgEz07JDI+Lh1KNuOR55bw1Wywhks5YnqR04AXgEaIuIrZAlDElHpGKT6D/CftIg8Z6CeNHnDzrqvre3l0VTd78330oD6lpp1Hu5Zmubk4FZBZIOAe4CLomINwc4rV9pJP1Q4/2DVYy6L5VKXPvQ2+/Nj/ao+npqpVHv5ZqtbT5NlNPeeR/t6TYVtm+TtD9ZIrgtIu5O4W19Ay7T6/YUrzTyfqD4UQVxs4ZxMjArk3r23AJsiojrcotWkY2Sh71Hy68C5qVeRdOBN9LppDXADEkT0oXjGcCatOwtSdPTZ83DI++twXyayKy/U4FzgackbUixbwBXAyslXQi8CJydlq0GZpHdsv0d4HyAiNgh6Uqy27EAXBERfbeDv4jsWSEHAfenyaxhnAzMykTEQxSf1wc4vaB8AAsrbGspsLQgvh44roZqmo0onyYyMzMnAzMzczIwMzOcDMzMDCcDMzPDycDMzHAyMDMzqkgGkpZK2i7p6Vzs7yQ9m+7dfo+k8SneLunXkjak6fu5dQrv317pHvFmZlY/1RwZ3Er/e613AcdFxCeBXwKX5ZY9HxHHp+nLuXil+7dXuke8mZnVyaDJICIeBHaUxX4aEbvS7Dr2vulWP4Pcv73SPeLNzKxORuKawQXsfV+VyZJ+Lul/S/rjFBvo/u173SMeOAIzM6urmu5NJOmvyR4ReFsKbQU+EhGvSToJ+JGkYxnC/dsH+bxBH/RRZNHU7CAmX77aWK2a7QEWtWi1tpjZHsNOBunh3n8GnJ5O/RAR7wLvpvePS3oe+BgD3799m6Qj05Oj8veI76eaB30UOS89oyD/0I9qY7VqtgdY1KLV2mJmewzrNJGkmcClwF9ExDu5+OGSxqX3HyW7UPzCIPdvr3SPeDMzq5NBjwwk3Q50AIdJ6gEWk/UeOhDoSj1E16WeQ38CXCFpF7Ab+HIV92+vdI94MzOrk0GTQUTMLQjfUqHsXWSPCixaVnj/9oh4jYJ7xJuZWf14BLKZmTkZmJmZk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZgVkrRU0nZJT+dil0t6WdKGNM3KLbtMUrek5ySdkYvPTLFuSZ25+GRJj0jaLOkOSQfUr3Vm/TkZmBW7FZhZEL8+Io5P02oASccAc4Bj0zrfkzQuPdvjRuBM4BhgbioLcE3a1hRgJ3DhqLbGbBBOBmYFIuJBYMegBTOzgRUR8W5E/AroBk5OU3dEvBARvwVWALPTA55OA+5M6y8DzhrRBpgNkZOB2dBcLOnJdBppQopNAl7KlelJsUrxDwKvR8SusrhZwwz7Gchm+6CbgCuBSK/XAhcAKigbFP/YigHK9yNpAbAAoK2trfDZzb29vSyauvu9+e/elj05duqk/1SpHWNGb29vyz6vutna5mRgVqWI2Nb3XtLNwI/TbA9wdK7oUcAr6X1R/FVgvKT90tFBvnz5Zy4BlgBMmzYtOjo6+pUplUpc+9Db/eJbzulfdqwplUoUtbkVNFvbqjpNVKFnxURJXak3RFffIbMyN6TeE09KOjG3zvxUfrOk+bn4SZKeSuvckM6pmjUVSUfmZj8P9H0fVgFzJB0oaTIwBXgUeAyYknoOHUB2kXlVRATwAPCFtP584N56tMGskmqvGdxK/54VncDa1BtibZqHrOfElDQtIDu0RtJEYDFwCtmFtcW5c643pbJ96xX14jCrG0m3Aw8DH5fUI+lC4NvpR8uTwKeBrwJExEZgJfAM8BNgYUTsTr/6LwbWAJuAlakswKXA1yR1k11DKHyuuFm9VHWaKCIelNReFp4NdKT3y4AS2Q4+G1iefv2skzQ+/aLqALoiYgeApC5gpqQScGhEPJziy8l6Vtw/3EaNpPbO+wDYcvXnGlwTq6eImFsQrvgfdkRcBVxVEF8NrC6Iv0D2o8isKdRyzaAtIrYCRMRWSUek+FB7VkxK78vj/VRzMa3IoqlZp418+VpiQ9FsF4lq0WptMbM9RuMCcqWeEkON9w9WcTGtyHl9v+5zF9RqiQ1Fs10kqkWrtcXM9qhlnMG2vgtq6XV7ilfqWTFQ/KiCuJmZ1UktyWAVWS8I2Ls3xCpgXupVNB14I51OWgPMkDQhXTieAaxJy96SND31IpqHe1aYmdVVVaeJUs+KDuAwST1kvYKuBlamXhYvAmen4quBWWRD8t8BzgeIiB2SriTrbgdwRd/FZOAish5LB5FdOG6Ki8dmZvuKansTFfWsADi9oGwACytsZymwtCC+HjiumrqYmdnI872JzMzMycDMzJwMzMwMJwMzM8PJwMzMcDIwMzNaMBm0d9733s3lzMysOi2XDMzMbOicDMzMzMnAzMycDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM2pIBpI+LmlDbnpT0iWSLpf0ci4+K7fOZZK6JT0n6YxcfGaKdUvqrLVRZmY2NFU9A7lIRDwHHA8gaRzwMnAPcD5wfUR8J19e0jHAHOBY4MPAzyR9LC2+Efgs0AM8JmlVRDwz3LqZmdnQDDsZlDkdeD4i/l1SpTKzgRUR8S7wK0ndwMlpWXdEvAAgaUUq62RgZlYnI5UM5gC35+YvljQPWA8sioidwCRgXa5MT4oBvFQWP6XoQyQtABYAtLW1USqV+pVZNHUXwF7LRjo2FL29vcNet9m0WlvMbI+ak4GkA4C/AC5LoZuAK4FIr9cCFwBFhwxB8XWLKPqsiFgCLAGYNm1adHR09CtzXnqWwZZzOkYtNhSlUomieo5FrdYWM9tjJHoTnQk8ERHbACJiW0TsjojfATez51RQD3B0br2jgFcGiJs1jKSlkrZLejoXmyipS9Lm9DohxSXphtQB4klJJ+bWmZ/Kb5Y0Pxc/SdJTaZ0bNMD5VbN6GIlkMJfcKSJJR+aWfR7o+zKtAuZIOlDSZGAK8CjwGDBF0uR0lDEnlTVrpFuBmWWxTmBtREwB1qZ5yH4QTUnTArKjYyRNBBaTnfY8GVjcl0BSmQW59co/y6yuakoGkn6PrBfQ3bnwt9MvnieBTwNfBYiIjcBKsgvDPwEWpiOIXcDFwBpgE7AylTVrmIh4ENhRFp4NLEvvlwFn5eLLI7MOGJ9+FJ0BdEXEjnTdrAuYmZYdGhEPR0QAy3PbMmuImq4ZRMQ7wAfLYucOUP4q4KqC+GpgdS11MauDtojYChARWyUdkeKT6N8JYtIg8Z6CeD/VdJro7e1l0dTd/eKtcF2klTotlGu2to1Ub6J9SnvfReWrP9fgmliTqNQ5Yqjx/sEqOk2USiWufejtfvHhdnpoJq3UaaFcs7XNt6Mwq962vmti6XV7ig+1c0RPel8eN2sYJwOz6q0C+noEzQfuzcXnpV5F04E30umkNcAMSRPSheMZwJq07C1J01Mvonm5bZk1hE8TmRWQdDvQARwmqYesV9DVwEpJFwIvAmen4quBWUA38A7ZLVmIiB2SriTrMQdwRUT0XZS+iKzH0kHA/WkyaxgnA7MCETG3wqLTC8oGsLDCdpYCSwvi64Hjaqmj2UjyaSIzM3MyMDMzJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzPDycCsZfXdQ8usGk4GZmbmZGBmZk4GZmaGk4GZmeFkYGZmjEAykLQlPfN4g6T1KTZRUpekzel1QopL0g2SuiU9KenE3Hbmp/KbJc2v9HlmZjbyRurI4NMRcXxETEvzncDaiJgCrE3zAGcCU9K0ALgJsuRBdr/4U4CTgcV9CcTMzEbfaJ0mmg0sS++XAWfl4ssjsw4Ynx4feAbQFRE7ImIn0AXMHKW6mZlZmZF4uE0AP5UUwD+mB3i3pUf7ERFbJR2Ryk4CXsqt25NileJ7kbSA7IiCtrY2SqVSv8osmroLYK9l9YhV0tvbW1W5saDV2mJme4xEMjg1Il5J/+F3SXp2gLIqiMUA8b0DWaJZAjBt2rTo6Ojot9J5adTllnM66hqrpFQqUVTPsajV2mJme9R8migiXkmv24F7yM75b0unf0iv21PxHuDo3OpHAa8MEDczszqoKRlIOljSB/reAzOAp4FVQF+PoPnAven9KmBe6lU0HXgjnU5aA8yQNCFdOJ6RYmZmVge1niZqA+6R1LetH0bETyQ9BqyUdCHwInB2Kr8amAV0A+8A5wNExA5JVwKPpXJXRMSOGutmZmZVqikZRMQLwB8VxF8DTi+IB7CwwraWAktrqY+ZmQ2PRyCPkPbO+3zLYDMbs5wMzMzMycDMzJwMzMwMJwMzM8PJwMzMcDIwMzOcDMyGzM/wsFbkZGA2PH6Gh7UUJwOzkeFneNiYNhK3sDbb1zTVMzx6e3tZNHV3YUXH+q26W+kZGuWarW1OBmZD11TP8CiVSlz70NuFH17NMzeaWSs9Q6Ncs7XNp4nMhmgsPcPD98yyajkZmA2Bn+FhrcqnicyGxs/wsJbkZGA2BH6Gh7UqnyYaRe2d9/HUy280uhpmZoNyMjAzs+EnA0lHS3pA0iZJGyV9JcUvl/RyGqq/QdKs3DqXpWH5z0k6IxefmWLdkjqLPs/MzEZPLdcMdgGLIuKJ1LvicUldadn1EfGdfGFJxwBzgGOBDwM/k/SxtPhG4LNk3e0ek7QqIp6poW5mZjYEw04GqXtc34jLtyRtomAEZc5sYEVEvAv8SlI3Wf9sgO50YQ5JK1JZJwMzszoZkd5EktqBE4BHgFOBiyXNA9aTHT3sJEsU63Kr5Yfflw/LP6XC5ww6NH/R1F3A3sPwGxlrO2js3xKgT7MNn69Fb29vo6tg1lRqTgaSDgHuAi6JiDcl3QRcSTa0/krgWuACKg+/L7pu0W9YPlQ3NP+8NNoyPwy/kbFFU3fxxSYacl6LZhs+X4tWSWpmI6WmZCBpf7JEcFtE3A0QEdtyy28GfpxmBxp+P+rD8s3MrLJaehMJuAXYFBHX5eJH5op9nmyoPmTD8udIOlDSZLL7uz9KNgJziqTJkg4gu8i8arj1MjOzoavlyOBU4FzgKUkbUuwbwFxJx5Od6tkCfAkgIjZKWkl2YXgXsDAidgNIupjsvizjgKURsbGGepmZ2RDV0pvoIYqvA6weYJ2rgKsK4qsHWq+V9N1BcsvVn2twTWxf4v3OBuMRyGZm5mRgZmZOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgdk+pW+8gVk5J4Mm0N55n7+kZtZQTgZmZuZkYGZmTgZmZoaTgdk+x9eorIiTQZPyF9bM6snJwMzMnAzM9lU++rQ8J4MxxF9eMxstTgZm+zj/wDBoomQgaaak5yR1S+psdH3GCh8tjF3NtM97P7KmSAaSxgE3AmcCxwBzJR3T2FqNff5yN69m3ef7koL3nX3Pfo2uQHIy0B0RLwBIWgHMBp5paK1aUHvnff0eil4e88PT66Lp9/lqE4L3k9agiGh0HZD0BWBmRPz3NH8ucEpEXFxWbgGwIM1+HHiurhUdnsOAVxtdiRHSam05OCIOb8SHj/A+30p/l3Ju28j7/aL9vlmODFQQ65elImIJsGT0qzNyJK2PiGmNrsdIaMG2tDeyCgWxYe3zrfR3Kee21U9TXDMAeoCjc/NHAa80qC5m9eB93ppKsySDx4ApkiZLOgCYA6xqcJ3MRpP3eWsqTXGaKCJ2SboYWAOMA5ZGxMYGV2ukjKnTWoNwW0bICO/zrfR3Kee21UlTXEA2M7PGapbTRGZm1kBOBmZm5mQwkiQtlbRd0tO52ERJXZI2p9cJjaxjNSQdLekBSZskbZT0lRQfi215v6RHJf0iteVvUnyypEdSW+5IF3HHnGa6pcVwSdoi6SlJGyStT7HCfU2ZG1J7n5R0YmNrv8dQvv8DtUPS/FR+s6T59aq/k8HIuhWYWRbrBNZGxBRgbZpvdruARRHxCWA6sDDdKmEstuVd4LSI+CPgeGCmpOnANcD1qS07gQsbWMdhadZbWgzTpyPi+Fy/+0r72pnAlDQtAG6qe00ru5Xqv/+F7ZA0EVgMnEI2Sn1xvX50ORmMoIh4ENhRFp4NLEvvlwFn1bVSwxARWyPiifT+LWATMImx2ZaIiN40u3+aAjgNuDPFx0RbCrx3S4uI+C3Qd0uLVlBpX5sNLE9/13XAeElHNqKC5Yb4/a/UjjOArojYERE7gS76J5hR4WQw+toiYitk/8kCRzS4PkMiqR04AXiEMdoWSeMkbQC2k325ngdej4hdqUgPWbIbayYBL+Xmx2o7AvippMfT7Teg8r421to81HY0rH1NMc7AmpOkQ4C7gEsi4k2p6A4KzdOiVuEAAAF+SURBVC8idgPHSxoP3AN8oqhYfWs1Iqq6pcUYcGpEvCLpCKBL0rMDlG2VNldqR8Pa5yOD0bet7zA2vW5vcH2qIml/skRwW0TcncJjsi19IuJ1oER2HWS8pL4fQ2P1VhAtcUuLiHglvW4nS9YnU3lfG2ttHmo7GtY+J4PRtwro6xEwH7i3gXWpirJDgFuATRFxXW7RWGzL4emIAEkHAZ8huwbyAPCFVGxMtKXAmL+lhaSDJX2g7z0wA3iayvvaKmBe6o0zHXij7zRMkxpqO9YAMyRNSBeOZ6TY6IsITyM0AbcDW4H/R5bhLwQ+SNaLYHN6ndjoelbRjk+RHZo+CWxI06wx2pZPAj9PbXka+B8p/lHgUaAb+GfgwEbXdZjtmwX8kuw6yF83uj7DqP9HgV+kaWNfGyrta2SnUW5M7X0KmNboNuTaUvX3f6B2ABek/bIbOL9e9fftKMzMzKeJzMzMycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzM+D/A4DEbLfOqD9eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. We can fix the maximum length of the reviews to 80 since that seems to be the majority review length. Similarly, we can set the maximum summary length to 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=80 \n",
    "max_len_summary=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting closer to the model building part. Before that, we need to split our dataset into a training and validation set. We’ll use 90% of the dataset as the training data and evaluate the performance on the remaining 10% (holdout set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(data['cleaned_text'],data['cleaned_summary'],test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Tokenizer\n",
    "A tokenizer builds the vocabulary and converts a word sequence to an integer sequence. Go ahead and build tokenizers for text and summary:\n",
    "\n",
    "#### a) Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index) +1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
    "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_voc_size  =   len(y_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "We are finally at the model building part. But before we do that, we need to familiarize ourselves with a few terms which are required prior to building the model.\n",
    "\n",
    "<li>Return Sequences = True: When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep</li>\n",
    "\n",
    "<li>Return State = True: When return state = True, LSTM produces the hidden state and cell state of the last timestep only</li>\n",
    "\n",
    "<li>Initial State: This is used to initialize the internal states of the LSTM for the first timestep</li>\n",
    "\n",
    "<li>Stacked LSTM: Stacked LSTM has multiple layers of LSTM stacked on top of each other. This leads to a better representation of the sequence. I encourage you to experiment with the multiple layers of the LSTM stacked on top of each other (it’s a great way to learn this)</li>\n",
    "Here, we are building a 3 stacked LSTM for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
